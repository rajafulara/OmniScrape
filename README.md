# ğŸ•·ï¸ OmniScrape - Professional Web Scraper

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.52+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

**A powerful, user-friendly web scraping tool built with Python and Streamlit**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Screenshots](#screenshots) â€¢ [Contributing](#contributing)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration Options](#configuration-options)
- [Data Export](#data-export)
- [Technical Details](#technical-details)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)
- [License](#license)

---

## ğŸ¯ Overview

**OmniScrape** is a comprehensive web scraping application that provides an intuitive interface for extracting structured data from websites. Built with Streamlit, it offers real-time scraping, data visualization, and easy export capabilitiesâ€”all without writing a single line of code.

Perfect for:
- ğŸ“Š Data analysts gathering web data
- ğŸ”¬ Researchers collecting online information
- ğŸ’¼ Business professionals conducting competitive analysis
- ğŸ“ Students learning web scraping techniques

---

## âœ¨ Features

### Core Capabilities

- **ğŸ”— Link Extraction**: Automatically identifies and categorizes internal and external links
- **ğŸ–¼ï¸ Image Discovery**: Extracts all images with alt text and source URLs
- **ğŸ“ Text Content Parsing**: Captures text from customizable HTML tags (h1, h2, p, li, etc.)
- **ğŸ“Š Table Extraction**: Detects and converts HTML tables to structured data
- **ğŸ“§ Email Discovery**: Uses regex to find email addresses on pages
- **â„¹ï¸ Metadata Collection**: Gathers page title, description, encoding, and server info

### User Experience

- **ğŸ¨ Beautiful UI**: Clean, modern interface with responsive design
- **âš¡ Real-time Progress**: Live progress indicators and status updates
- **ğŸ“± Multi-Device Support**: Simulates Desktop, Mobile, and Tablet user agents
- **ğŸ’¾ Export Options**: Download data as CSV files for further analysis
- **ğŸ” Image Preview**: Visual gallery of discovered images
- **ğŸ› ï¸ Customizable**: Configure target tags and user agents

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Step 1: Clone or Download the Project

```bash
cd "Python Web Scraper"
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv venv
.\venv\Scripts\Activate.ps1

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies include:**
- `streamlit` - Web application framework
- `requests` - HTTP library for making web requests
- `beautifulsoup4` - HTML/XML parsing
- `pandas` - Data manipulation and analysis
- `lxml` - Fast XML/HTML parser

---

## ğŸ’» Usage

### Running the Application

1. **Activate your virtual environment** (if not already active):
   ```bash
   # Windows
   .\venv\Scripts\Activate.ps1
   
   # macOS/Linux
   source venv/bin/activate
   ```

2. **Start the Streamlit server**:
   ```bash
   streamlit run main.py
   ```

3. **Access the application**:
   - The app will automatically open in your default browser
   - Default URL: `http://localhost:8501`

### Quick Start Guide

1. **Enter URL**: Type or paste the website URL you want to scrape
2. **Configure Settings** (Sidebar):
   - Select user agent (Desktop/Mobile/Tablet)
   - Choose target HTML tags for text extraction
3. **Click "Start Scraping"**: Begin the extraction process
4. **View Results**: Browse through different tabs to see extracted data
5. **Download Data**: Export any dataset as CSV for analysis

---

## âš™ï¸ Configuration Options

### User Agent Selection

Choose how your scraper identifies itself:
- **Desktop**: Standard desktop browser (Chrome)
- **Mobile**: iPhone Safari
- **Tablet**: iPad Chrome

### Target Tags for Text Extraction

Customize which HTML elements to extract text from:
- `h1`, `h2`, `h3`, `h4` - Headers
- `p` - Paragraphs
- `li` - List items
- `span` - Inline text
- `div` - Division elements

---

## ğŸ“¥ Data Export

All extracted data can be downloaded as CSV files:

| Data Type | File Name | Contents |
|-----------|-----------|----------|
| Links | `links.csv` | Link text, URL, type (internal/external) |
| Images | `images.csv` | Alt text, source URL |
| Text | `content.csv` | HTML tag, text content |
| Emails | `emails.csv` | Discovered email addresses |
| Tables | `table_1.csv` | Extracted table data |

---

## ğŸ”§ Technical Details

### Architecture

```
OmniScrape/
â”‚
â”œâ”€â”€ main.py                 # Main application file
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â””â”€â”€ venv/                  # Virtual environment (created locally)
```

### Key Components

**WebScraper Class**
- Handles HTTP requests with custom headers
- Parses HTML using BeautifulSoup
- Extracts and structures data
- Manages errors and timeouts

**Streamlit Interface**
- Responsive layout with sidebar controls
- Tabbed data visualization
- Progress tracking and status updates
- Download functionality

### Technologies Used

- **Python 3.13+**: Core programming language
- **Streamlit**: Web application framework
- **Beautiful Soup 4**: HTML parsing
- **Pandas**: Data manipulation
- **Requests**: HTTP client
- **lxml**: Fast HTML/XML processing

---

## ğŸ“Œ Best Practices

### Ethical Scraping

âœ… **DO:**
- Check and respect `robots.txt` files
- Add delays between requests for large-scale scraping
- Use appropriate user agents
- Comply with website Terms of Service
- Cache data to minimize repeated requests

âŒ **DON'T:**
- Overload servers with rapid requests
- Scrape personal or sensitive data
- Ignore copyright and data protection laws
- Use scraped data for malicious purposes

### Performance Tips

- Start with smaller pages to test
- Use mobile user agent for faster responses
- Limit target tags to reduce processing time
- Clear browser cache if running into issues

---

## ğŸ› Troubleshooting

### Common Issues

**Problem**: "Failed to scrape URL" error
- **Solution**: Check if the URL is accessible in your browser, verify internet connection, try different user agent

**Problem**: No data extracted
- **Solution**: The website might use JavaScript to load content (requires Selenium/Playwright for dynamic content)

**Problem**: Streamlit won't start
- **Solution**: Ensure virtual environment is activated and all dependencies are installed

**Problem**: Execution policy error (Windows)
- **Solution**: Run PowerShell as Administrator and execute:
  ```powershell
  Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
  ```

### Getting Help

If you encounter issues:
1. Check the terminal output for error messages
2. Verify all dependencies are correctly installed
3. Ensure you're using Python 3.8+
4. Try running with a simple, well-known website first

---

## ğŸ“„ License

This project is licensed under the MIT License - feel free to use, modify, and distribute as needed.

---

## ğŸ™ Acknowledgments

- Built with [Streamlit](https://streamlit.io/)
- HTML parsing by [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/)
- Data handling with [Pandas](https://pandas.pydata.org/)

---

## ğŸ“ Contact & Support

For questions, suggestions, or contributions:
- Open an issue on the repository
- Submit a pull request with improvements

---

<div align="center">

**â­ If you find this project useful, please consider giving it a star! â­**

Made with â¤ï¸ using Python & Streamlit

</div>
